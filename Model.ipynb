{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is the modeling part of  Santander Customer Transaction Prediction Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tried:\n",
    " - Randomness seed, doesn't work\n",
    " - Variable bins\n",
    " - Narrow prossibilities\n",
    " - Add count in test for each instance\n",
    " - Resampling\n",
    " - Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "# Any results you write to the current directory are saved as output.\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "df_train = pd.read_pickle('./train.pkl')\n",
    "df_test = pd.read_pickle('./test.pkl')\n",
    "y = df_train['target']#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features\n",
    "feature = [col for col in df_train.columns if col not in ['ID_code', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and labels\n",
    "public = np.load('public_LB.npy')\n",
    "private = np.load('private_LB.npy')\n",
    "real_idx = np.hstack([public,private])\n",
    "real_test = df_test.iloc[real_idx,:]\n",
    "fake_idx = np.array(list(set(range(200000))-set(real_idx)))\n",
    "fake_test = df_test.iloc[fake_idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_raw = df_train[feature]\n",
    "test_X_raw = real_test[feature]\n",
    "fake_X_raw = fake_test[feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_noise = np.load('train_0408_noise_raw.npy')\n",
    "test_X_noise = np.load('test_0408_noise_raw.npy')\n",
    "fake_X_noise = np.load('train_fake_raw.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise_merge = np.vstack([train_X_noise, test_X_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = noise_merge[:,1].copy()\n",
    "# n_unique = len(np.unique(x, return_counts=True)[1])\n",
    "# # cluster = int(n_unique*2/3)\n",
    "# fake_idx = np.where(np.isnan(x))[0]\n",
    "# mask = np.ones(x.shape[0], dtype=bool)\n",
    "# mask[fake_idx] = False\n",
    "# clean_data = x[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried different ways to denoise and smooth data, which took too much time, so not implemented in the final model, but the idea is to use some clustering method and polyorder smooth method to denoise and smooth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(df):\n",
    "    for feature in tqdm_notebook(range(df.shape[1])):\n",
    "        x = df[:,feature].copy()\n",
    "        n_unique = len(np.unique(x, return_counts=True)[1])\n",
    "        cluster = int(n_unique*2/3)\n",
    "        fake_idx = np.where(np.isnan(x))[0]\n",
    "        mask = np.ones(x.shape[0], dtype=bool)\n",
    "        mask[fake_idx] = False\n",
    "        clean_data = x[mask]\n",
    "        kmeans = KMeans(n_clusters=cluster, random_state=0, n_jobs=16,max_iter=100).fit(clean_data.reshape(-1, 1))\n",
    "        x[mask] = kmeans.labels_\n",
    "        df[:,feature] = x\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(df):\n",
    "    for feature in tqdm_notebook(range(df.shape[1])):\n",
    "        x = df[:,feature].copy()\n",
    "        n_unique = len(np.unique(x, return_counts=True)[1])\n",
    "#         cluster = int(n_unique*2/3)\n",
    "        fake_idx = np.where(np.isnan(x))[0]\n",
    "        mask = np.ones(x.shape[0], dtype=bool)\n",
    "        mask[fake_idx] = False\n",
    "        clean_data = x[mask]\n",
    "        x[mask] = savgol_filter(clean_data,window_length=5,polyorder=3)\n",
    "        df[:,feature] = x\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_merge = denoise(noise_merge)\n",
    "# smooth_merge = smooth(noise_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_noise = noise_merge[:200000]\n",
    "# test_X_noise = noise_merge[200000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_noise = smooth_merge[:200000]\n",
    "# test_X_noise = smooth_merge[200000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cnt_train = np.load('nan_cnt_train.npy')\n",
    "nan_cnt_test = np.load('nan_cnt_test.npy')\n",
    "var_sta_train = np.load('var_sta_train.npy')\n",
    "var_sta_test = np.load('var_sta_test.npy')\n",
    "var_sta_fake = np.load('var_sta_fake.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine different features\n",
    "train_X = np.hstack([train_X_raw,\n",
    "                     train_X_noise,\n",
    "#                      train_X_sta,\n",
    "                     var_sta_train,\n",
    "                     np.expand_dims(nan_cnt_train,1)])\n",
    "\n",
    "test_X = np.hstack([test_X_raw,\n",
    "                    test_X_noise,\n",
    "#                     test_X_sta,\n",
    "                    var_sta_test,\n",
    "                    np.expand_dims(nan_cnt_test,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_cnt_fake = np.isnan(fake_X_noise).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get features for fake data\n",
    "# fake_X = np.hstack([fake_X_raw,\n",
    "#                     fake_X_noise,\n",
    "# #                     test_X_sta,\n",
    "#                     var_sta_fake,\n",
    "#                     np.expand_dims(nan_cnt_fake,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try use some rough bins method to denoise data\n",
    "for i in range(200):\n",
    "    train_X[:,200+i] = np.round(train_X[:,200+i], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold split and use first fold to make quick check\n",
    "num_folds = 5\n",
    "folds = StratifiedKFold(n_splits=num_folds, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = list(folds.split(train_X, y))[0][0]\n",
    "val_idx = list(folds.split(train_X, y))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn, y_train = train_X[train_idx], y[train_idx]\n",
    "X_valid, y_valid = train_X[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = lgb.Dataset(X_trn, label=y_train)\n",
    "val_data = lgb.Dataset(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# ros = RandomOverSampler(random_state=0)\n",
    "# X_resampled, y_resampled = ros.fit_resample(X_trn, y_train)\n",
    "# trn_data = lgb.Dataset(X_resampled, label=y_resampled)\n",
    "# val_data = lgb.Dataset(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use Lightgbm since it's fast, I tried different method for data preprocessing, different groups of features and different hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.35,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.3,\n",
    "    'learning_rate': 0.0083,\n",
    "    'max_depth': 8,\n",
    "    'metric': 'auc',\n",
    "    'max_bin': 165,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample':0.7,\n",
    "#     'subsample':0.4,\n",
    "    'num_leaves': 2,\n",
    "    'colsample_bytree': 0.03,\n",
    "    'num_threads': 36,\n",
    "    'tree_learner': 'serial',\n",
    "#     'lambda_l1' : 1.7,\n",
    "#     'lambda_l2' : 5,\n",
    "    'objective': 'binary', \n",
    "    'verbosity': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_freq': 5, 'bagging_fraction': 0.5, 'boost_from_average': 'false', 'boost': 'gbdt', 'feature_fraction': 0.3, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'auc', 'max_bin': 180, 'min_data_in_leaf': 40, 'min_child_weight': 10, 'subsample': 0.7, 'num_leaves': 2, 'colsample_bytree': 0.03, 'num_threads': 36, 'tree_learner': 'serial', 'objective': 'binary', 'verbosity': -1}\n",
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[2500]\ttraining's auc: 0.862672\tvalid_1's auc: 0.851922\n",
      "[5000]\ttraining's auc: 0.890861\tvalid_1's auc: 0.878785\n",
      "[7500]\ttraining's auc: 0.905086\tvalid_1's auc: 0.892467\n",
      "[10000]\ttraining's auc: 0.913406\tvalid_1's auc: 0.900568\n",
      "[12500]\ttraining's auc: 0.918728\tvalid_1's auc: 0.905626\n",
      "[15000]\ttraining's auc: 0.922527\tvalid_1's auc: 0.909293\n",
      "[17500]\ttraining's auc: 0.925369\tvalid_1's auc: 0.912002\n",
      "[20000]\ttraining's auc: 0.927587\tvalid_1's auc: 0.914064\n",
      "[22500]\ttraining's auc: 0.929353\tvalid_1's auc: 0.91575\n",
      "[25000]\ttraining's auc: 0.930646\tvalid_1's auc: 0.916949\n",
      "[27500]\ttraining's auc: 0.93174\tvalid_1's auc: 0.917732\n",
      "[30000]\ttraining's auc: 0.932704\tvalid_1's auc: 0.918504\n",
      "[32500]\ttraining's auc: 0.933527\tvalid_1's auc: 0.918968\n",
      "[35000]\ttraining's auc: 0.934217\tvalid_1's auc: 0.91937\n",
      "[37500]\ttraining's auc: 0.934844\tvalid_1's auc: 0.919734\n",
      "[40000]\ttraining's auc: 0.935421\tvalid_1's auc: 0.919976\n",
      "[42500]\ttraining's auc: 0.935924\tvalid_1's auc: 0.92013\n",
      "[45000]\ttraining's auc: 0.936405\tvalid_1's auc: 0.920328\n",
      "[47500]\ttraining's auc: 0.936832\tvalid_1's auc: 0.92044\n",
      "Early stopping, best iteration is:\n",
      "[47702]\ttraining's auc: 0.936863\tvalid_1's auc: 0.92047\n"
     ]
    }
   ],
   "source": [
    "# scale rd4 +pct975\n",
    "print(param)\n",
    "clf = lgb.train(param, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=2500, early_stopping_rounds = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_freq': 5, 'bagging_fraction': 0.35, 'boost_from_average': 'false', 'boost': 'gbdt', 'feature_fraction': 0.4, 'learning_rate': 0.0083, 'max_depth': 8, 'metric': 'auc', 'max_bin': 165, 'min_data_in_leaf': 40, 'min_child_weight': 10, 'subsample': 0.7, 'num_leaves': 2, 'colsample_bytree': 0.03, 'num_threads': 36, 'tree_learner': 'serial', 'objective': 'binary', 'verbosity': -1}\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[2500]\ttraining's auc: 0.856842\tvalid_1's auc: 0.846069\n",
      "[5000]\ttraining's auc: 0.886017\tvalid_1's auc: 0.874494\n",
      "[7500]\ttraining's auc: 0.90104\tvalid_1's auc: 0.889096\n",
      "[10000]\ttraining's auc: 0.909948\tvalid_1's auc: 0.89805\n",
      "[12500]\ttraining's auc: 0.915796\tvalid_1's auc: 0.903417\n",
      "[15000]\ttraining's auc: 0.920141\tvalid_1's auc: 0.907553\n",
      "[17500]\ttraining's auc: 0.923099\tvalid_1's auc: 0.910466\n",
      "[20000]\ttraining's auc: 0.925569\tvalid_1's auc: 0.912835\n",
      "[22500]\ttraining's auc: 0.927541\tvalid_1's auc: 0.914735\n",
      "[25000]\ttraining's auc: 0.929016\tvalid_1's auc: 0.916017\n",
      "[27500]\ttraining's auc: 0.930273\tvalid_1's auc: 0.91721\n",
      "[30000]\ttraining's auc: 0.931293\tvalid_1's auc: 0.918096\n",
      "[32500]\ttraining's auc: 0.932214\tvalid_1's auc: 0.918936\n",
      "[35000]\ttraining's auc: 0.932982\tvalid_1's auc: 0.919334\n",
      "[37500]\ttraining's auc: 0.933602\tvalid_1's auc: 0.919772\n",
      "[40000]\ttraining's auc: 0.934164\tvalid_1's auc: 0.92002\n",
      "[42500]\ttraining's auc: 0.934655\tvalid_1's auc: 0.920246\n",
      "[45000]\ttraining's auc: 0.935151\tvalid_1's auc: 0.920372\n",
      "[47500]\ttraining's auc: 0.93558\tvalid_1's auc: 0.920517\n",
      "[50000]\ttraining's auc: 0.936002\tvalid_1's auc: 0.920636\n",
      "[52500]\ttraining's auc: 0.936395\tvalid_1's auc: 0.920575\n",
      "[55000]\ttraining's auc: 0.936766\tvalid_1's auc: 0.920439\n",
      "Early stopping, best iteration is:\n",
      "[51020]\ttraining's auc: 0.936158\tvalid_1's auc: 0.920661\n"
     ]
    }
   ],
   "source": [
    "# scale rd4 +pct975\n",
    "print(param)\n",
    "clf = lgb.train(param, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=2500, early_stopping_rounds = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_freq': 5, 'bagging_fraction': 0.35, 'boost_from_average': 'false', 'boost': 'gbdt', 'feature_fraction': 0.4, 'learning_rate': 0.0083, 'max_depth': -1, 'metric': 'auc', 'max_bin': 180, 'min_data_in_leaf': 40, 'min_child_weight': 10, 'subsample': 0.7, 'num_leaves': 2, 'colsample_bytree': 0.03, 'num_threads': -1, 'tree_learner': 'serial', 'objective': 'binary', 'verbosity': -1}\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[2500]\ttraining's auc: 0.856812\tvalid_1's auc: 0.846718\n",
      "[5000]\ttraining's auc: 0.88632\tvalid_1's auc: 0.874853\n",
      "[7500]\ttraining's auc: 0.901324\tvalid_1's auc: 0.889252\n",
      "[10000]\ttraining's auc: 0.909998\tvalid_1's auc: 0.89799\n",
      "[12500]\ttraining's auc: 0.915968\tvalid_1's auc: 0.9035\n",
      "[15000]\ttraining's auc: 0.920292\tvalid_1's auc: 0.907717\n",
      "[17500]\ttraining's auc: 0.923285\tvalid_1's auc: 0.910555\n",
      "[20000]\ttraining's auc: 0.925723\tvalid_1's auc: 0.912847\n",
      "[22500]\ttraining's auc: 0.927763\tvalid_1's auc: 0.914834\n",
      "[25000]\ttraining's auc: 0.929208\tvalid_1's auc: 0.916076\n",
      "[27500]\ttraining's auc: 0.93048\tvalid_1's auc: 0.917223\n",
      "[30000]\ttraining's auc: 0.931496\tvalid_1's auc: 0.918087\n",
      "[32500]\ttraining's auc: 0.932431\tvalid_1's auc: 0.918871\n",
      "[35000]\ttraining's auc: 0.933207\tvalid_1's auc: 0.919322\n",
      "[37500]\ttraining's auc: 0.933815\tvalid_1's auc: 0.919683\n",
      "[40000]\ttraining's auc: 0.934407\tvalid_1's auc: 0.919907\n",
      "[42500]\ttraining's auc: 0.934925\tvalid_1's auc: 0.920147\n",
      "[45000]\ttraining's auc: 0.935427\tvalid_1's auc: 0.920301\n",
      "[47500]\ttraining's auc: 0.935856\tvalid_1's auc: 0.920464\n",
      "[50000]\ttraining's auc: 0.93628\tvalid_1's auc: 0.92057\n",
      "[52500]\ttraining's auc: 0.936676\tvalid_1's auc: 0.92053\n",
      "[55000]\ttraining's auc: 0.937037\tvalid_1's auc: 0.920356\n",
      "Early stopping, best iteration is:\n",
      "[51394]\ttraining's auc: 0.936506\tvalid_1's auc: 0.920591\n"
     ]
    }
   ],
   "source": [
    "# scale rd4 +pct975\n",
    "print(param)\n",
    "clf = lgb.train(param, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=2500, early_stopping_rounds = 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {'tree_method': 'hist',\n",
    "             'objective': 'binary:logistic',\n",
    "             'eval_metric': 'auc',\n",
    "             'learning_rate': 0.01,\n",
    "#              'max_depth': 5,\n",
    "             'colsample_bytree': 0.03,\n",
    "             'subsample': 0.8,\n",
    "             'min_child_weight': 20,\n",
    "             'gamma': 1.2,\n",
    "#              'silent': 1,\n",
    "             'n_jobs':16,\n",
    "             'n_estimators': 20,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_trn, label=y_train)\n",
    "dval = xgb.DMatrix(X_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_trn, y_train)\n",
    "dval = xgb.DMatrix(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=2,\n",
    "                          learning_rate=0.01,\n",
    "                          n_estimators=100000,\n",
    "                          min_child_weight=40,\n",
    "                          subsample=0.5,\n",
    "                          colsample_bytree=0.04,\n",
    "                          tree_method='hist',\n",
    "                          colsample_bylevel=0.5,\n",
    "#                           min_child_weight=30,\n",
    "                          scale_pos_weight=2,\n",
    "                          gamma=9,\n",
    "                          max_bin=150,\n",
    "                          n_jobs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.526323\tvalidation_1-auc:0.528088\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.87009\tvalidation_1-auc:0.857147\n",
      "[2000]\tvalidation_0-auc:0.8873\tvalidation_1-auc:0.872382\n",
      "[3000]\tvalidation_0-auc:0.900403\tvalidation_1-auc:0.88449\n",
      "[4000]\tvalidation_0-auc:0.909287\tvalidation_1-auc:0.892416\n",
      "[5000]\tvalidation_0-auc:0.915592\tvalidation_1-auc:0.89847\n",
      "[6000]\tvalidation_0-auc:0.920553\tvalidation_1-auc:0.903009\n",
      "[7000]\tvalidation_0-auc:0.924206\tvalidation_1-auc:0.906186\n",
      "[8000]\tvalidation_0-auc:0.927245\tvalidation_1-auc:0.90883\n",
      "[9000]\tvalidation_0-auc:0.929781\tvalidation_1-auc:0.910995\n",
      "[10000]\tvalidation_0-auc:0.931731\tvalidation_1-auc:0.912657\n",
      "[11000]\tvalidation_0-auc:0.933449\tvalidation_1-auc:0.914049\n",
      "[12000]\tvalidation_0-auc:0.934908\tvalidation_1-auc:0.915139\n",
      "[13000]\tvalidation_0-auc:0.936175\tvalidation_1-auc:0.916092\n",
      "[14000]\tvalidation_0-auc:0.937314\tvalidation_1-auc:0.916824\n",
      "[15000]\tvalidation_0-auc:0.938321\tvalidation_1-auc:0.917485\n",
      "[16000]\tvalidation_0-auc:0.939244\tvalidation_1-auc:0.918056\n",
      "[17000]\tvalidation_0-auc:0.940045\tvalidation_1-auc:0.918518\n",
      "[18000]\tvalidation_0-auc:0.9408\tvalidation_1-auc:0.918877\n",
      "[19000]\tvalidation_0-auc:0.941478\tvalidation_1-auc:0.919193\n",
      "[20000]\tvalidation_0-auc:0.942098\tvalidation_1-auc:0.91941\n",
      "[21000]\tvalidation_0-auc:0.942661\tvalidation_1-auc:0.919619\n",
      "[22000]\tvalidation_0-auc:0.943182\tvalidation_1-auc:0.919727\n",
      "[23000]\tvalidation_0-auc:0.943727\tvalidation_1-auc:0.919856\n",
      "[24000]\tvalidation_0-auc:0.944199\tvalidation_1-auc:0.919931\n",
      "[25000]\tvalidation_0-auc:0.944676\tvalidation_1-auc:0.919996\n",
      "[26000]\tvalidation_0-auc:0.945137\tvalidation_1-auc:0.919978\n",
      "Stopping. Best iteration:\n",
      "[25272]\tvalidation_0-auc:0.944806\tvalidation_1-auc:0.920012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_clf=model.fit(X_trn, y_train,\n",
    "            eval_set=[(X_trn, y_train), (X_valid, y_valid)],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=1000,\n",
    "            verbose=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n",
       "       colsample_bytree=0.04, gamma=9, learning_rate=0.01, max_bin=150,\n",
       "       max_delta_step=0, max_depth=2, min_child_weight=40, missing=None,\n",
       "       n_estimators=100000, n_jobs=15, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=2, seed=None, silent=True,\n",
       "       subsample=0.5, tree_method='hist')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Blending to see if the performance got improved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_predict = clf.predict(X_valid)\n",
    "xgb_predict = xgb_clf.predict_proba(X_valid)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9201445940028325"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true=y_valid, y_score=lgb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9200115808771933"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true=y_valid, y_score=xgb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9202838258611061"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_true=y_valid, y_score=(lgb_predict+ xgb_predict)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the validation dataset, there is a small boost after blending, so I tried two models to blend, but for the final submission, it turns out my best submission is only got by single LightGBM model in the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five Fold CV Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold idx:1\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[2500]\ttraining's auc: 0.856295\tvalid_1's auc: 0.845521\n",
      "[5000]\ttraining's auc: 0.885992\tvalid_1's auc: 0.8746\n",
      "[7500]\ttraining's auc: 0.900693\tvalid_1's auc: 0.888716\n",
      "[10000]\ttraining's auc: 0.909713\tvalid_1's auc: 0.897667\n",
      "[12500]\ttraining's auc: 0.915754\tvalid_1's auc: 0.903398\n",
      "[15000]\ttraining's auc: 0.919988\tvalid_1's auc: 0.907517\n",
      "[17500]\ttraining's auc: 0.922953\tvalid_1's auc: 0.910469\n",
      "[20000]\ttraining's auc: 0.925452\tvalid_1's auc: 0.912786\n",
      "[22500]\ttraining's auc: 0.927405\tvalid_1's auc: 0.914635\n",
      "[25000]\ttraining's auc: 0.928878\tvalid_1's auc: 0.915967\n",
      "[27500]\ttraining's auc: 0.930134\tvalid_1's auc: 0.917156\n",
      "[30000]\ttraining's auc: 0.931159\tvalid_1's auc: 0.918049\n",
      "[32500]\ttraining's auc: 0.932104\tvalid_1's auc: 0.91892\n",
      "[35000]\ttraining's auc: 0.932868\tvalid_1's auc: 0.919327\n",
      "[37500]\ttraining's auc: 0.933486\tvalid_1's auc: 0.919787\n",
      "[40000]\ttraining's auc: 0.934074\tvalid_1's auc: 0.920062\n",
      "[42500]\ttraining's auc: 0.934563\tvalid_1's auc: 0.920234\n",
      "[45000]\ttraining's auc: 0.935048\tvalid_1's auc: 0.920374\n",
      "[47500]\ttraining's auc: 0.935485\tvalid_1's auc: 0.920523\n",
      "[50000]\ttraining's auc: 0.935886\tvalid_1's auc: 0.92065\n",
      "[52500]\ttraining's auc: 0.936274\tvalid_1's auc: 0.920593\n",
      "Early stopping, best iteration is:\n",
      "[49911]\ttraining's auc: 0.935869\tvalid_1's auc: 0.920652\n",
      "Fold idx:2\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[2500]\ttraining's auc: 0.855491\tvalid_1's auc: 0.847791\n",
      "[5000]\ttraining's auc: 0.885163\tvalid_1's auc: 0.876419\n",
      "[7500]\ttraining's auc: 0.900523\tvalid_1's auc: 0.8908\n",
      "[10000]\ttraining's auc: 0.909681\tvalid_1's auc: 0.899251\n",
      "[12500]\ttraining's auc: 0.915774\tvalid_1's auc: 0.904762\n",
      "[15000]\ttraining's auc: 0.919931\tvalid_1's auc: 0.908408\n",
      "[17500]\ttraining's auc: 0.923113\tvalid_1's auc: 0.911285\n",
      "[20000]\ttraining's auc: 0.925663\tvalid_1's auc: 0.913565\n",
      "[22500]\ttraining's auc: 0.927583\tvalid_1's auc: 0.91489\n",
      "[25000]\ttraining's auc: 0.929117\tvalid_1's auc: 0.916159\n",
      "[27500]\ttraining's auc: 0.930345\tvalid_1's auc: 0.917085\n",
      "[30000]\ttraining's auc: 0.931392\tvalid_1's auc: 0.917689\n",
      "[32500]\ttraining's auc: 0.93236\tvalid_1's auc: 0.91825\n",
      "[35000]\ttraining's auc: 0.933086\tvalid_1's auc: 0.918748\n",
      "[37500]\ttraining's auc: 0.933707\tvalid_1's auc: 0.919039\n",
      "[40000]\ttraining's auc: 0.934299\tvalid_1's auc: 0.919357\n",
      "[42500]\ttraining's auc: 0.934784\tvalid_1's auc: 0.919562\n",
      "[45000]\ttraining's auc: 0.935272\tvalid_1's auc: 0.919654\n",
      "[47500]\ttraining's auc: 0.935692\tvalid_1's auc: 0.919723\n",
      "[50000]\ttraining's auc: 0.936083\tvalid_1's auc: 0.919799\n",
      "[52500]\ttraining's auc: 0.936446\tvalid_1's auc: 0.919798\n",
      "[55000]\ttraining's auc: 0.936799\tvalid_1's auc: 0.919796\n",
      "[57500]\ttraining's auc: 0.937166\tvalid_1's auc: 0.919716\n",
      "Early stopping, best iteration is:\n",
      "[53768]\ttraining's auc: 0.936613\tvalid_1's auc: 0.919825\n",
      "Fold idx:3\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[2500]\ttraining's auc: 0.854154\tvalid_1's auc: 0.848166\n",
      "[5000]\ttraining's auc: 0.883809\tvalid_1's auc: 0.87895\n",
      "[7500]\ttraining's auc: 0.899173\tvalid_1's auc: 0.894515\n",
      "[10000]\ttraining's auc: 0.908368\tvalid_1's auc: 0.903841\n",
      "[12500]\ttraining's auc: 0.91425\tvalid_1's auc: 0.909475\n",
      "[15000]\ttraining's auc: 0.918711\tvalid_1's auc: 0.913688\n",
      "[17500]\ttraining's auc: 0.922065\tvalid_1's auc: 0.916557\n",
      "[20000]\ttraining's auc: 0.924619\tvalid_1's auc: 0.918662\n",
      "[22500]\ttraining's auc: 0.926606\tvalid_1's auc: 0.920199\n",
      "[25000]\ttraining's auc: 0.928171\tvalid_1's auc: 0.921361\n",
      "[27500]\ttraining's auc: 0.929389\tvalid_1's auc: 0.922092\n",
      "[30000]\ttraining's auc: 0.930441\tvalid_1's auc: 0.92273\n",
      "[32500]\ttraining's auc: 0.931367\tvalid_1's auc: 0.9232\n",
      "[35000]\ttraining's auc: 0.932175\tvalid_1's auc: 0.923698\n",
      "[37500]\ttraining's auc: 0.932855\tvalid_1's auc: 0.923956\n",
      "[40000]\ttraining's auc: 0.933431\tvalid_1's auc: 0.924157\n",
      "[42500]\ttraining's auc: 0.933922\tvalid_1's auc: 0.924324\n",
      "[45000]\ttraining's auc: 0.934401\tvalid_1's auc: 0.924483\n",
      "[47500]\ttraining's auc: 0.934826\tvalid_1's auc: 0.924456\n",
      "[50000]\ttraining's auc: 0.935212\tvalid_1's auc: 0.924482\n",
      "[52500]\ttraining's auc: 0.935604\tvalid_1's auc: 0.924503\n",
      "[55000]\ttraining's auc: 0.935966\tvalid_1's auc: 0.924479\n",
      "[57500]\ttraining's auc: 0.93633\tvalid_1's auc: 0.924523\n",
      "[60000]\ttraining's auc: 0.936661\tvalid_1's auc: 0.924382\n",
      "Early stopping, best iteration is:\n",
      "[56218]\ttraining's auc: 0.936154\tvalid_1's auc: 0.924562\n",
      "Fold idx:4\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[2500]\ttraining's auc: 0.854721\tvalid_1's auc: 0.848298\n",
      "[5000]\ttraining's auc: 0.884298\tvalid_1's auc: 0.877755\n",
      "[7500]\ttraining's auc: 0.899656\tvalid_1's auc: 0.893156\n",
      "[10000]\ttraining's auc: 0.908712\tvalid_1's auc: 0.901917\n",
      "[12500]\ttraining's auc: 0.914949\tvalid_1's auc: 0.90738\n",
      "[15000]\ttraining's auc: 0.919228\tvalid_1's auc: 0.911186\n",
      "[17500]\ttraining's auc: 0.922417\tvalid_1's auc: 0.913959\n",
      "[20000]\ttraining's auc: 0.924895\tvalid_1's auc: 0.916018\n",
      "[22500]\ttraining's auc: 0.926865\tvalid_1's auc: 0.917578\n",
      "[25000]\ttraining's auc: 0.928365\tvalid_1's auc: 0.918942\n",
      "[27500]\ttraining's auc: 0.929671\tvalid_1's auc: 0.919932\n",
      "[30000]\ttraining's auc: 0.930728\tvalid_1's auc: 0.920615\n",
      "[32500]\ttraining's auc: 0.931641\tvalid_1's auc: 0.921242\n",
      "[35000]\ttraining's auc: 0.932462\tvalid_1's auc: 0.921698\n",
      "[37500]\ttraining's auc: 0.933095\tvalid_1's auc: 0.922083\n",
      "[40000]\ttraining's auc: 0.9337\tvalid_1's auc: 0.922325\n",
      "[42500]\ttraining's auc: 0.934224\tvalid_1's auc: 0.922568\n",
      "[45000]\ttraining's auc: 0.934723\tvalid_1's auc: 0.922654\n",
      "[47500]\ttraining's auc: 0.935169\tvalid_1's auc: 0.922716\n",
      "[50000]\ttraining's auc: 0.935571\tvalid_1's auc: 0.922673\n",
      "[52500]\ttraining's auc: 0.935967\tvalid_1's auc: 0.922623\n",
      "Early stopping, best iteration is:\n",
      "[49025]\ttraining's auc: 0.935425\tvalid_1's auc: 0.92276\n",
      "Fold idx:5\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[2500]\ttraining's auc: 0.853664\tvalid_1's auc: 0.855811\n",
      "[5000]\ttraining's auc: 0.884064\tvalid_1's auc: 0.883282\n",
      "[7500]\ttraining's auc: 0.899208\tvalid_1's auc: 0.896811\n",
      "[10000]\ttraining's auc: 0.908507\tvalid_1's auc: 0.905501\n",
      "[12500]\ttraining's auc: 0.914555\tvalid_1's auc: 0.91066\n",
      "[15000]\ttraining's auc: 0.918933\tvalid_1's auc: 0.914507\n",
      "[17500]\ttraining's auc: 0.922156\tvalid_1's auc: 0.917211\n",
      "[20000]\ttraining's auc: 0.924703\tvalid_1's auc: 0.919214\n",
      "[22500]\ttraining's auc: 0.926492\tvalid_1's auc: 0.920605\n",
      "[25000]\ttraining's auc: 0.92804\tvalid_1's auc: 0.921772\n",
      "[27500]\ttraining's auc: 0.929339\tvalid_1's auc: 0.922567\n",
      "[30000]\ttraining's auc: 0.930354\tvalid_1's auc: 0.923198\n",
      "[32500]\ttraining's auc: 0.931195\tvalid_1's auc: 0.92381\n",
      "[35000]\ttraining's auc: 0.931965\tvalid_1's auc: 0.924293\n",
      "[37500]\ttraining's auc: 0.932591\tvalid_1's auc: 0.924511\n",
      "[40000]\ttraining's auc: 0.933167\tvalid_1's auc: 0.924788\n",
      "[42500]\ttraining's auc: 0.933684\tvalid_1's auc: 0.924937\n",
      "[45000]\ttraining's auc: 0.934164\tvalid_1's auc: 0.925079\n",
      "[47500]\ttraining's auc: 0.934614\tvalid_1's auc: 0.925033\n",
      "Early stopping, best iteration is:\n",
      "[45060]\ttraining's auc: 0.93418\tvalid_1's auc: 0.925102\n"
     ]
    }
   ],
   "source": [
    "best_iteration=[]\n",
    "predictions = np.zeros(test_X.shape[0])\n",
    "folds = StratifiedKFold(n_splits=num_folds, random_state=random_state)\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, y)):\n",
    "    \n",
    "    X_tr, y_tr = train_X[trn_idx], y[trn_idx]\n",
    "    X_valid, y_valid = train_X[val_idx], y[val_idx]\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "    \n",
    "    clf = lgb.train(param, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=2500, early_stopping_rounds = 4000)\n",
    "    best_iteration.append(clf.best_iteration)\n",
    "    predictions += clf.predict(test_X, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_freq': 5,\n",
       " 'bagging_fraction': 0.35,\n",
       " 'boost_from_average': 'false',\n",
       " 'boost': 'gbdt',\n",
       " 'feature_fraction': 0.3,\n",
       " 'learning_rate': 0.0083,\n",
       " 'max_depth': 8,\n",
       " 'metric': 'auc',\n",
       " 'max_bin': 165,\n",
       " 'min_data_in_leaf': 40,\n",
       " 'min_child_weight': 10,\n",
       " 'subsample': 0.7,\n",
       " 'num_leaves': 2,\n",
       " 'colsample_bytree': 0.03,\n",
       " 'num_threads': 36,\n",
       " 'tree_learner': 'serial',\n",
       " 'objective': 'binary',\n",
       " 'verbosity': -1}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49911, 53768, 56218, 49025, 45060]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the 5-fold CV LightGBM prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lgb_prediction', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a31d542bce42fc99dc1f17c3a2caf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_prediction = np.zeros(test_X.shape[0])\n",
    "train_data = lgb.Dataset(train_X, label=y)\n",
    "for i,iteration in tqdm_notebook(enumerate(best_iteration)):\n",
    "    param = {\n",
    "    'bagging_freq': 5,\n",
    "    'feature_fraction_seed': i+828,\n",
    "    'bagging_seed': i+828,\n",
    "    'bagging_fraction': 0.35,\n",
    "    'boost_from_average': 'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.3,\n",
    "    'learning_rate': 0.0083,\n",
    "    'max_depth': 8,\n",
    "    'metric': 'auc',\n",
    "    'max_bin': 165,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample': 0.7,\n",
    "    'num_leaves': 2,\n",
    "    'colsample_bytree': 0.03,\n",
    "    'num_threads': 36,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary',\n",
    "    'verbosity': -1\n",
    "}\n",
    "    clf = lgb.train(param, train_data, iteration, verbose_eval=2500)\n",
    "    full_prediction += clf.predict(test_X) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-20ef7c668478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In case the fake data would also be scored, I used the lgbm model predict the fake data as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfake_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfake_prediction\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# In case the fake data would also be scored, I used the lgbm model predict the fake data as well\n",
    "fake_prediction = np.zeros(test_X.shape[0])\n",
    "fake_prediction += clf.predict(fake_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('fake_prediction',fake_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lgbm_full_prediction',full_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.551115\tvalidation_1-auc:0.553355\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.867385\tvalidation_1-auc:0.855691\n",
      "[2000]\tvalidation_0-auc:0.882694\tvalidation_1-auc:0.868559\n",
      "[3000]\tvalidation_0-auc:0.89422\tvalidation_1-auc:0.879347\n",
      "[4000]\tvalidation_0-auc:0.903675\tvalidation_1-auc:0.887903\n",
      "[5000]\tvalidation_0-auc:0.910523\tvalidation_1-auc:0.894233\n",
      "[6000]\tvalidation_0-auc:0.915842\tvalidation_1-auc:0.89912\n",
      "[7000]\tvalidation_0-auc:0.919933\tvalidation_1-auc:0.902689\n",
      "[8000]\tvalidation_0-auc:0.923303\tvalidation_1-auc:0.905637\n",
      "[9000]\tvalidation_0-auc:0.925967\tvalidation_1-auc:0.908014\n",
      "[10000]\tvalidation_0-auc:0.928274\tvalidation_1-auc:0.90994\n",
      "[11000]\tvalidation_0-auc:0.93024\tvalidation_1-auc:0.911549\n",
      "[12000]\tvalidation_0-auc:0.93198\tvalidation_1-auc:0.912863\n",
      "[13000]\tvalidation_0-auc:0.933431\tvalidation_1-auc:0.913969\n",
      "[14000]\tvalidation_0-auc:0.934767\tvalidation_1-auc:0.914919\n",
      "[15000]\tvalidation_0-auc:0.935897\tvalidation_1-auc:0.915689\n",
      "[16000]\tvalidation_0-auc:0.936955\tvalidation_1-auc:0.916395\n",
      "[17000]\tvalidation_0-auc:0.937901\tvalidation_1-auc:0.916982\n",
      "[18000]\tvalidation_0-auc:0.938784\tvalidation_1-auc:0.917509\n",
      "[19000]\tvalidation_0-auc:0.939566\tvalidation_1-auc:0.917973\n",
      "[20000]\tvalidation_0-auc:0.940337\tvalidation_1-auc:0.918332\n",
      "[21000]\tvalidation_0-auc:0.94103\tvalidation_1-auc:0.918652\n",
      "[22000]\tvalidation_0-auc:0.941699\tvalidation_1-auc:0.918891\n",
      "[23000]\tvalidation_0-auc:0.94231\tvalidation_1-auc:0.919166\n",
      "[24000]\tvalidation_0-auc:0.942897\tvalidation_1-auc:0.919398\n",
      "[25000]\tvalidation_0-auc:0.943454\tvalidation_1-auc:0.919531\n",
      "[26000]\tvalidation_0-auc:0.943981\tvalidation_1-auc:0.919662\n",
      "[27000]\tvalidation_0-auc:0.944484\tvalidation_1-auc:0.919799\n",
      "[28000]\tvalidation_0-auc:0.944979\tvalidation_1-auc:0.919955\n",
      "[29000]\tvalidation_0-auc:0.945456\tvalidation_1-auc:0.919966\n",
      "[30000]\tvalidation_0-auc:0.945935\tvalidation_1-auc:0.92002\n",
      "[31000]\tvalidation_0-auc:0.946391\tvalidation_1-auc:0.920082\n",
      "Stopping. Best iteration:\n",
      "[30612]\tvalidation_0-auc:0.946213\tvalidation_1-auc:0.920091\n",
      "\n",
      "Fold idx:1\n",
      "[0]\tvalidation_0-auc:0.545943\tvalidation_1-auc:0.548361\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.866758\tvalidation_1-auc:0.856455\n",
      "[2000]\tvalidation_0-auc:0.882083\tvalidation_1-auc:0.87113\n",
      "[3000]\tvalidation_0-auc:0.894088\tvalidation_1-auc:0.882078\n",
      "[4000]\tvalidation_0-auc:0.903669\tvalidation_1-auc:0.89061\n",
      "[5000]\tvalidation_0-auc:0.910527\tvalidation_1-auc:0.896794\n",
      "[6000]\tvalidation_0-auc:0.915931\tvalidation_1-auc:0.901232\n",
      "[7000]\tvalidation_0-auc:0.920001\tvalidation_1-auc:0.904586\n",
      "[8000]\tvalidation_0-auc:0.923425\tvalidation_1-auc:0.907333\n",
      "[9000]\tvalidation_0-auc:0.926209\tvalidation_1-auc:0.909521\n",
      "[10000]\tvalidation_0-auc:0.92854\tvalidation_1-auc:0.911271\n",
      "[11000]\tvalidation_0-auc:0.93044\tvalidation_1-auc:0.912649\n",
      "[12000]\tvalidation_0-auc:0.932162\tvalidation_1-auc:0.913863\n",
      "[13000]\tvalidation_0-auc:0.933706\tvalidation_1-auc:0.914928\n",
      "[14000]\tvalidation_0-auc:0.935066\tvalidation_1-auc:0.915775\n",
      "[15000]\tvalidation_0-auc:0.936243\tvalidation_1-auc:0.916512\n",
      "[16000]\tvalidation_0-auc:0.937289\tvalidation_1-auc:0.917102\n",
      "[17000]\tvalidation_0-auc:0.938222\tvalidation_1-auc:0.917604\n",
      "[18000]\tvalidation_0-auc:0.939061\tvalidation_1-auc:0.917952\n",
      "[19000]\tvalidation_0-auc:0.93988\tvalidation_1-auc:0.918311\n",
      "[20000]\tvalidation_0-auc:0.940612\tvalidation_1-auc:0.918629\n",
      "[21000]\tvalidation_0-auc:0.941306\tvalidation_1-auc:0.918913\n",
      "[22000]\tvalidation_0-auc:0.941925\tvalidation_1-auc:0.91905\n",
      "[23000]\tvalidation_0-auc:0.942529\tvalidation_1-auc:0.919232\n",
      "[24000]\tvalidation_0-auc:0.943088\tvalidation_1-auc:0.919346\n",
      "[25000]\tvalidation_0-auc:0.943643\tvalidation_1-auc:0.919498\n",
      "[26000]\tvalidation_0-auc:0.94418\tvalidation_1-auc:0.919593\n",
      "[27000]\tvalidation_0-auc:0.944663\tvalidation_1-auc:0.919695\n",
      "[28000]\tvalidation_0-auc:0.945147\tvalidation_1-auc:0.919786\n",
      "[29000]\tvalidation_0-auc:0.945635\tvalidation_1-auc:0.919811\n",
      "[30000]\tvalidation_0-auc:0.946075\tvalidation_1-auc:0.919857\n",
      "[31000]\tvalidation_0-auc:0.946534\tvalidation_1-auc:0.919812\n",
      "Stopping. Best iteration:\n",
      "[30046]\tvalidation_0-auc:0.9461\tvalidation_1-auc:0.919869\n",
      "\n",
      "Fold idx:2\n",
      "[0]\tvalidation_0-auc:0.541859\tvalidation_1-auc:0.538932\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.86558\tvalidation_1-auc:0.858325\n",
      "[2000]\tvalidation_0-auc:0.880916\tvalidation_1-auc:0.872848\n",
      "[3000]\tvalidation_0-auc:0.892721\tvalidation_1-auc:0.88442\n",
      "[4000]\tvalidation_0-auc:0.901779\tvalidation_1-auc:0.893211\n",
      "[5000]\tvalidation_0-auc:0.908822\tvalidation_1-auc:0.899698\n",
      "[6000]\tvalidation_0-auc:0.914531\tvalidation_1-auc:0.904943\n",
      "[7000]\tvalidation_0-auc:0.918813\tvalidation_1-auc:0.908802\n",
      "[8000]\tvalidation_0-auc:0.92219\tvalidation_1-auc:0.911531\n",
      "[9000]\tvalidation_0-auc:0.925039\tvalidation_1-auc:0.913882\n",
      "[10000]\tvalidation_0-auc:0.927351\tvalidation_1-auc:0.915762\n",
      "[11000]\tvalidation_0-auc:0.929396\tvalidation_1-auc:0.917298\n",
      "[12000]\tvalidation_0-auc:0.931115\tvalidation_1-auc:0.918514\n",
      "[13000]\tvalidation_0-auc:0.932623\tvalidation_1-auc:0.919531\n",
      "[14000]\tvalidation_0-auc:0.933985\tvalidation_1-auc:0.920397\n",
      "[15000]\tvalidation_0-auc:0.935171\tvalidation_1-auc:0.92115\n",
      "[16000]\tvalidation_0-auc:0.936241\tvalidation_1-auc:0.921719\n",
      "[17000]\tvalidation_0-auc:0.937211\tvalidation_1-auc:0.922209\n",
      "[18000]\tvalidation_0-auc:0.938081\tvalidation_1-auc:0.922648\n",
      "[19000]\tvalidation_0-auc:0.93888\tvalidation_1-auc:0.922967\n",
      "[20000]\tvalidation_0-auc:0.939644\tvalidation_1-auc:0.923259\n",
      "[21000]\tvalidation_0-auc:0.940338\tvalidation_1-auc:0.923506\n",
      "[22000]\tvalidation_0-auc:0.940951\tvalidation_1-auc:0.923734\n",
      "[23000]\tvalidation_0-auc:0.941553\tvalidation_1-auc:0.923926\n",
      "[24000]\tvalidation_0-auc:0.942114\tvalidation_1-auc:0.924066\n",
      "[25000]\tvalidation_0-auc:0.94268\tvalidation_1-auc:0.924154\n",
      "[26000]\tvalidation_0-auc:0.943198\tvalidation_1-auc:0.924337\n",
      "[27000]\tvalidation_0-auc:0.943708\tvalidation_1-auc:0.924399\n",
      "[28000]\tvalidation_0-auc:0.944207\tvalidation_1-auc:0.924467\n",
      "[29000]\tvalidation_0-auc:0.94473\tvalidation_1-auc:0.924458\n",
      "[30000]\tvalidation_0-auc:0.94519\tvalidation_1-auc:0.924487\n",
      "Stopping. Best iteration:\n",
      "[29872]\tvalidation_0-auc:0.945133\tvalidation_1-auc:0.924509\n",
      "\n",
      "Fold idx:3\n",
      "[0]\tvalidation_0-auc:0.54185\tvalidation_1-auc:0.542748\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.863201\tvalidation_1-auc:0.854695\n",
      "[2000]\tvalidation_0-auc:0.880409\tvalidation_1-auc:0.871432\n",
      "[3000]\tvalidation_0-auc:0.893151\tvalidation_1-auc:0.883145\n",
      "[4000]\tvalidation_0-auc:0.902435\tvalidation_1-auc:0.89178\n",
      "[5000]\tvalidation_0-auc:0.909636\tvalidation_1-auc:0.898368\n",
      "[6000]\tvalidation_0-auc:0.914872\tvalidation_1-auc:0.903119\n",
      "[7000]\tvalidation_0-auc:0.919114\tvalidation_1-auc:0.906611\n",
      "[8000]\tvalidation_0-auc:0.922637\tvalidation_1-auc:0.909329\n",
      "[9000]\tvalidation_0-auc:0.925391\tvalidation_1-auc:0.911475\n",
      "[10000]\tvalidation_0-auc:0.927686\tvalidation_1-auc:0.913302\n",
      "[11000]\tvalidation_0-auc:0.929663\tvalidation_1-auc:0.91481\n",
      "[12000]\tvalidation_0-auc:0.931328\tvalidation_1-auc:0.916088\n",
      "[13000]\tvalidation_0-auc:0.932845\tvalidation_1-auc:0.917009\n",
      "[14000]\tvalidation_0-auc:0.93418\tvalidation_1-auc:0.917942\n",
      "[15000]\tvalidation_0-auc:0.935354\tvalidation_1-auc:0.918646\n",
      "[16000]\tvalidation_0-auc:0.936395\tvalidation_1-auc:0.919307\n",
      "[17000]\tvalidation_0-auc:0.937367\tvalidation_1-auc:0.919889\n",
      "[18000]\tvalidation_0-auc:0.938243\tvalidation_1-auc:0.920289\n",
      "[19000]\tvalidation_0-auc:0.939052\tvalidation_1-auc:0.920706\n",
      "[20000]\tvalidation_0-auc:0.939814\tvalidation_1-auc:0.920996\n",
      "[21000]\tvalidation_0-auc:0.940487\tvalidation_1-auc:0.921239\n",
      "[22000]\tvalidation_0-auc:0.941161\tvalidation_1-auc:0.921481\n",
      "[23000]\tvalidation_0-auc:0.941762\tvalidation_1-auc:0.921689\n",
      "[24000]\tvalidation_0-auc:0.942375\tvalidation_1-auc:0.921895\n",
      "[25000]\tvalidation_0-auc:0.942944\tvalidation_1-auc:0.922054\n",
      "[26000]\tvalidation_0-auc:0.94347\tvalidation_1-auc:0.922141\n",
      "[27000]\tvalidation_0-auc:0.943982\tvalidation_1-auc:0.922211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28000]\tvalidation_0-auc:0.944482\tvalidation_1-auc:0.922331\n",
      "[29000]\tvalidation_0-auc:0.944997\tvalidation_1-auc:0.922376\n",
      "[30000]\tvalidation_0-auc:0.945459\tvalidation_1-auc:0.922347\n",
      "Stopping. Best iteration:\n",
      "[29159]\tvalidation_0-auc:0.945067\tvalidation_1-auc:0.922393\n",
      "\n",
      "Fold idx:4\n",
      "[0]\tvalidation_0-auc:0.543228\tvalidation_1-auc:0.547292\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 1000 rounds.\n",
      "[1000]\tvalidation_0-auc:0.862746\tvalidation_1-auc:0.862918\n",
      "[2000]\tvalidation_0-auc:0.879937\tvalidation_1-auc:0.877538\n",
      "[3000]\tvalidation_0-auc:0.893121\tvalidation_1-auc:0.889152\n",
      "[4000]\tvalidation_0-auc:0.902447\tvalidation_1-auc:0.89667\n",
      "[5000]\tvalidation_0-auc:0.909566\tvalidation_1-auc:0.902762\n",
      "[6000]\tvalidation_0-auc:0.914836\tvalidation_1-auc:0.907155\n",
      "[7000]\tvalidation_0-auc:0.919031\tvalidation_1-auc:0.910466\n",
      "[8000]\tvalidation_0-auc:0.922392\tvalidation_1-auc:0.913155\n",
      "[9000]\tvalidation_0-auc:0.925183\tvalidation_1-auc:0.915048\n",
      "[10000]\tvalidation_0-auc:0.927497\tvalidation_1-auc:0.916615\n",
      "[11000]\tvalidation_0-auc:0.929518\tvalidation_1-auc:0.918075\n",
      "[12000]\tvalidation_0-auc:0.9312\tvalidation_1-auc:0.919189\n",
      "[13000]\tvalidation_0-auc:0.932747\tvalidation_1-auc:0.92018\n",
      "[14000]\tvalidation_0-auc:0.934093\tvalidation_1-auc:0.920942\n",
      "[15000]\tvalidation_0-auc:0.935231\tvalidation_1-auc:0.921567\n",
      "[16000]\tvalidation_0-auc:0.93629\tvalidation_1-auc:0.922137\n",
      "[17000]\tvalidation_0-auc:0.93726\tvalidation_1-auc:0.922631\n",
      "[18000]\tvalidation_0-auc:0.93811\tvalidation_1-auc:0.923015\n",
      "[19000]\tvalidation_0-auc:0.938942\tvalidation_1-auc:0.923335\n",
      "[20000]\tvalidation_0-auc:0.939689\tvalidation_1-auc:0.923596\n",
      "[21000]\tvalidation_0-auc:0.940353\tvalidation_1-auc:0.923774\n",
      "[22000]\tvalidation_0-auc:0.940998\tvalidation_1-auc:0.923937\n",
      "[23000]\tvalidation_0-auc:0.941618\tvalidation_1-auc:0.924138\n",
      "[24000]\tvalidation_0-auc:0.942204\tvalidation_1-auc:0.924285\n",
      "[25000]\tvalidation_0-auc:0.942743\tvalidation_1-auc:0.924389\n",
      "[26000]\tvalidation_0-auc:0.94325\tvalidation_1-auc:0.924512\n",
      "[27000]\tvalidation_0-auc:0.94377\tvalidation_1-auc:0.924575\n",
      "[28000]\tvalidation_0-auc:0.944281\tvalidation_1-auc:0.92455\n",
      "Stopping. Best iteration:\n",
      "[27288]\tvalidation_0-auc:0.943917\tvalidation_1-auc:0.924589\n",
      "\n",
      "Fold idx:5\n"
     ]
    }
   ],
   "source": [
    "best_iteration_xgb=[]\n",
    "predictions = np.zeros(test_X.shape[0])\n",
    "folds = StratifiedKFold(n_splits=num_folds, random_state=random_state)\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, y)):\n",
    "    \n",
    "    X_tr, y_tr = train_X[trn_idx], y[trn_idx]\n",
    "    X_valid, y_valid = train_X[val_idx], y[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(max_depth=2,\n",
    "                          learning_rate=0.0083,\n",
    "                          n_estimators=100000,\n",
    "                          min_child_weight=40,\n",
    "                          subsample=0.5,\n",
    "                          colsample_bytree=0.04,\n",
    "                          tree_method='hist',\n",
    "                          colsample_bylevel=0.5,\n",
    "#                           min_child_weight=30,\n",
    "                          scale_pos_weight=2,\n",
    "                          gamma=5,\n",
    "                          max_bin=150,\n",
    "                          n_jobs=36)\n",
    "    \n",
    "    xgb_clf=model.fit(X_tr, y_tr,\n",
    "            eval_set=[(X_tr, y_tr), (X_valid, y_valid)],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=1000,\n",
    "            verbose=1000)\n",
    "    \n",
    "    print(\"Fold idx:{}\".format(fold_ + 1))\n",
    "    best_iteration_xgb.append(xgb_clf.best_iteration)\n",
    "    predictions += xgb_clf.predict_proba(test_X)[:,1] / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xgb_prediction', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_full_prediction = np.zeros(test_X.shape[0])\n",
    "# train_data = lgb.Dataset(train_X, label=y)\n",
    "model = xgb.XGBClassifier(max_depth=2,\n",
    "                      learning_rate=0.0083,\n",
    "                      n_estimators=100000,\n",
    "                      min_child_weight=40,\n",
    "                      subsample=0.5,\n",
    "                      colsample_bytree=0.04,\n",
    "                      tree_method='hist',\n",
    "                      colsample_bylevel=0.5,\n",
    "#                           min_child_weight=30,\n",
    "                      scale_pos_weight=3,\n",
    "                      gamma=9,\n",
    "                      max_bin=150,\n",
    "                      n_jobs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = np.load('xgb_prediction.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_full_prediction = np.load('lgbm_full_prediction.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_cv = np.load('lgb_prediction.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Full Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_full_prediction = np.zeros(test_X.shape[0])\n",
    "# train_data = lgb.Dataset(train_X, label=y)\n",
    "model = xgb.XGBClassifier(max_depth=2,\n",
    "                      learning_rate=0.0083,\n",
    "                      n_estimators=30000,\n",
    "                      min_child_weight=40,\n",
    "                      subsample=0.5,\n",
    "                      colsample_bytree=0.04,\n",
    "                      tree_method='hist',\n",
    "                      colsample_bylevel=0.5,\n",
    "#                           min_child_weight=30,\n",
    "                      scale_pos_weight=3,\n",
    "                      gamma=9,\n",
    "                      max_bin=150,\n",
    "                      n_jobs=15)\n",
    "\n",
    "xgb_clf=model.fit(train_X, y)\n",
    "    \n",
    "xgb_full_prediction += xgb_clf.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xgb_full_prediction',xgb_full_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble all predictions and check the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'lgbm_full': lgbm_full_prediction,\n",
    "                       'lgbm_cv': lgbm_cv,\n",
    "                       'xgb_cv': xgb_cv,\n",
    "                       'xgb_full': xgb_full_prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm_cv</th>\n",
       "      <th>lgbm_full</th>\n",
       "      <th>xgb_cv</th>\n",
       "      <th>xgb_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm_cv</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999529</td>\n",
       "      <td>0.985214</td>\n",
       "      <td>0.964282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_full</th>\n",
       "      <td>0.999529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985024</td>\n",
       "      <td>0.964269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_cv</th>\n",
       "      <td>0.985214</td>\n",
       "      <td>0.985024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_full</th>\n",
       "      <td>0.964282</td>\n",
       "      <td>0.964269</td>\n",
       "      <td>0.994143</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lgbm_cv  lgbm_full    xgb_cv  xgb_full\n",
       "lgbm_cv    1.000000   0.999529  0.985214  0.964282\n",
       "lgbm_full  0.999529   1.000000  0.985024  0.964269\n",
       "xgb_cv     0.985214   0.985024  1.000000  0.994143\n",
       "xgb_full   0.964282   0.964269  0.994143  1.000000"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "public = np.load('public_LB.npy')\n",
    "private = np.load('private_LB.npy')\n",
    "real_idx = np.hstack([public,private])\n",
    "real_test = df_test.iloc[real_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = lgbm_full_prediction*0.55 + xgb_full_prediction*0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID_code\": df_test.ID_code.values})\n",
    "submission[\"target\"] = 0\n",
    "submission.set_value(fake_idx,'target', fake_prediction)\n",
    "submission.set_value(real_idx,'target', final_prediction)\n",
    "submission.to_csv(\"submission_final_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 6.03M/6.03M [00:03<00:00, 1.69MB/s]\n",
      "Successfully submitted to Santander Customer Transaction Prediction"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit santander-customer-transaction-prediction -f submission_final_full.csv -m \"My submission message\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
